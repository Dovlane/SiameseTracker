import matplotlib.pyplot as plt

#Adam
#train_loss_per_epoch=[1850.2725040279329, 1333.5408776644617, 1163.892444591038, 1068.3313015424646,
#                      999.4748412081972, 948.8031542822719, 914.2788548329845, 883.2172065847553, 858.6732123747934, 838.0040563298389,819.2753281531623, 805.4474033951992, 
#                      790.6286227672826, 776.5355698587373, 768.0945795804728, 757.8654267258244, 748.3357562255114, 741.6879364391789, 733.877092723269, 726.917859365698
#                     ]

#val_loss_per_epoch=[505.46918400377035, 828.7395267784595, 607.2581806778908, 597.7116660475731, 568.5281563848257, 652.9719646200538, 622.4780061617494, 603.3808075860143,
#                     625.1115451529622, 838.4220744296908,740.9038185402751, 875.4403315484524, 653.0238537937403, 938.8497474491596, 687.7826415672898, 556.1366221681237,
#                     583.4353117123246, 655.0630235746503, 562.2858478650451, 569.6309831812978]

#center_error_train=[27.5968, 24.7075, 23.7604, 23.2287, 22.8699, 22.6038, 22.4046, 22.2300, 22.1040, 21.9742, 21.9132, 21.7889, 21.7348, 21.6809, 21.6285, 21.5739, 21.5000, 21.4658, 21.4084, 21.3614]
#center_error_val=[24.0841, 23.0403, 23.1877, 22.4467, 22.4886, 22.3388, 22.5298, 22.8235, 22.1301, 22.2350, 22.5749, 22.2691, 22.5860, 22.2376, 21.9333, 22.2021, 22.5683, 22.5892, 22.8294, 22.2781]

#SGD

#center_error_val=[20.6350, 20.8315, 20.8637,20.7515, 20.5396,21.0318,20.7109, 20.2374,
#  20.3149,21.3010, 20.9762, 20.7711, 21.1043,20.8159, 20.7161,20.4358,
#    20.9487, 21.6494, 21.2698, 20.9708, ]

#val_loss=[402.7956319861114, 436.8319863155484, 433.9097976908088,
#           504.8352354541421, 427.1556105017662, 461.0249320752919,
#             418.14606683701277, 419.94393353164196, 402.2764468193054,
#               458.8510539717972, 446.56974970176816, 549.5538698509336,
#                 494.9537823945284, 538.6330678388476, 486.10357728227973,
#                   426.2314570918679, 582.048305567354, 637.0803352110088,
#                     634.9632157459855, 495.44968984648585]

#train_loss=[1610.2970122136176, 1284.3472205195576, 1181.4465107601136,
#             1113.8714054543525, 1062.1576383933425, 1024.4467757716775,
#               984.2180379126221, 959.2201892947778, 930.6809450746514,
#                 909.2110604820773, 888.4620159000624, 870.9635044839233,
#                   853.8835208832752, 838.3090965941083, 825.6871891051996,
#                     811.7028955430724, 802.3132282467559, 791.872610892402,
#                       781.9784800261259, 770.677569268737]

#center_error_train=[24.9742, 23.7876, 23.2834, 22.9090, 22.6107,
#                    22.3894, 22.2281, 22.0628, 21.9068, 21.8070, 21.7155,
#                      21.6170, 21.4945,21.4086,21.3517, 21.3049, 21.2306,
#                        21.1445, 21.1119, 21.0658]

#augmented
val_loss_per_epoch=[554.8378848880529, 643.9979449957609, 621.4401867240667, 854.0545235723257, 583.9205741882324, 544.4839274287224, 511.2697857916355, 461.02786584198475, 499.076404646039, 626.4827375262976]
train_loss_per_epoch=[1909.0616455078125, 1388.6912261508405, 1210.5968074128032, 1105.9536413163878, 1039.2076025363058, 987.584167141933, 947.0910458620638, 915.9017891071271, 891.8331724151503, 869.0174409630708]

#train_loss_per_epoch=[x/11736 for x in train_loss_per_epoch]
#val_loss_per_epoch=[x/2456 for x in val_loss_per_epoch]


train_loss_per_epoch=[x/11736 for x in train_loss_per_epoch]
val_loss_per_epoch=[x/2456 for x in val_loss_per_epoch]

plt.figure(figsize=(10, 6))
plt.plot(train_loss_per_epoch, marker='o', linestyle='-', color='b', label='Train loss')
plt.plot(val_loss_per_epoch, marker='o', linestyle='-', color='g', label='Validation loss')
plt.title('Training and validation loss per epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""   mls01
SloganCinnamonArrange
-sgd pusti, lr=0.03
-vizualizuj rezultate,

"""